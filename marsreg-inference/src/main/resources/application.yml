server:
  port: 8084

spring:
  application:
    name: marsreg-inference
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848

# LLM配置
llm:
  provider: openai
  model: gpt-3.5-turbo
  temperature: 0.7
  max-tokens: 2000
  timeout: 30
  retry:
    max-attempts: 3
    initial-interval: 1000
    multiplier: 2
    max-interval: 10000

# 推理配置
inference:
  context:
    max-tokens: 4000
    max-documents: 5
  prompt:
    template-path: classpath:prompts/
  cache:
    enabled: true
    ttl: 3600 