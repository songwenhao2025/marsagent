server:
  port: 8083

spring:
  application:
    name: marsreg-inference
  cloud:
    nacos:
      discovery:
        server-addr: localhost:8848
  redis:
    host: localhost
    port: 6379
    database: 0
    timeout: 10000
    lettuce:
      pool:
        max-active: 8
        max-wait: -1
        max-idle: 8
        min-idle: 0

# 监控配置
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  endpoint:
    health:
      show-details: always
    metrics:
      enabled: true
    prometheus:
      enabled: true

# LLM配置
llm:
  provider: openai
  model: gpt-3.5-turbo
  temperature: 0.7
  max-tokens: 2000
  timeout: 30
  retry:
    max-attempts: 3
    initial-interval: 1000
    multiplier: 2
    max-interval: 10000

# 推理配置
inference:
  context:
    max-documents: 10
    max-tokens: 2000
    min-similarity: 0.7
  model:
    name: gpt-3.5-turbo
    version: 1.0.0
    temperature: 0.7
    max-tokens: 2000
    timeout: 30
    parameters:
      top_p: 1.0
      frequency_penalty: 0.0
      presence_penalty: 0.0
  cache:
    enabled: true
    expire-seconds: 3600
    max-size: 1000
  retry:
    max-attempts: 3
    initial-delay: 1000
    max-delay: 10000
    multiplier: 2
  prompt:
    template-path: classpath:prompts/

marsreg:
  inference:
    cache:
      enabled: true
      max-size: 1000
      expire-seconds: 3600
    metrics:
      enabled: true
      retention-days: 30
      export-to-prometheus: true 